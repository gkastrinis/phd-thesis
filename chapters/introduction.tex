\chapter{Introduction}
\label{chapter:intro}
\epigraph{Donâ€™t be scared. All of this is new to you, and new can be scary. Now we all want answers. Stick with me---you might get some.}{\textit{The 13th Doctor} - Doctor Who}

Static program analysis is the cornerstone of several modern programming facilities and tools for program development and aided program understanding. Nowadays, it is an umbrella term for many different methodologies (Hoare logic~\cite{article:1969:Hoare,sam:1967:Floyd,lics:2002:Reynolds,csl:2001:OHearn}, model checking~\cite{icalp:1980:Emerson,lop:1981:Clarke,toplas:1986:Clarke,isp:1982:Queille}, symbolic execution~\cite{article:1976:King,article:1977:Howden,icas:2010:Pasareanu,article:1975:Boyer}, abstract interpretation~\cite{popl:1977:Cousot,article:1992:Cousot-LP,article:1992:Cousot-LC}, data-flow analysis~\cite{popl:1973:Kildall,book:2009:Khedker,book:1997:Muchnick,article:1977:Kam,popl:1995:Reps,book:1981:Sharir}, and so on) all with the ultimate goal of inferring a program's properties, without the need of an actual execution. It is routinely employed in many different contexts: compilers, bug detectors, verifiers, security analyzers, IDEs, and a myriad other tools.

The main intention of any \emph{static} program analysis algorithm is to reason about the set of all feasible behaviors (under some abstraction of behaviors) that a given program might exhibit under all possible executions. For example, could this method throw a runtime exception? or is that type cast possible to fail under some program input? etc. As a result, virtually all interesting static program analysis questions are undecidable---indeed the prototypical undecidable problem, the \emph{halting problem}, is a static program analysis question: will a program terminate under all inputs?

Pointer analysis (also known as \emph{points-to analysis}) is a fundamental subdomain of static program analysis that consists of computing some \emph{abstract memory model} for a given program. The essence of such an analysis is to compute a set of possible objects that a program variable or expression may point to during program execution. A straightforward endeavor at first, it quickly gets too complicated in practice due to all of the intricate details one has to take into account and the multitude of different features that mutually depend on each other.\footnote{The analysis inputs are large and the analysis algorithms are typically quadratic or cubic, but try to maintain near-linear behavior in practice, by exploiting program properties and maintaining precision---more precise (i.e., smaller) inference sets lead to less work.} Although a challenging task, smart implementations of pointer analysis can bear many benefits to client analyses that will subsequently consume the results to reason about specialized behaviors (e.g., security vulnerabilities or potential optimization opportunities).

A closely related analysis, sometimes confused with pointer analysis, is \emph{alias analysis} in which one computes sets of program expressions that may alias (i.e., point to common objects) with each other. Pointer analysis could---although it is not the only possible alternative---be used to implement an alias analysis algorithm, and vice versa.

At the same time, programming languages are evolving, becoming ever higher-level and more complex. Many abstraction levels are added throughout the years with the aim of making the very task of programming easier for developers allowing them to express more with less effort (e.g., in terms of lines of code). Frequently, new features come with complicated semantics regarding their possible implementations and usually they interact in intricate ways with pre-existing ones.

Additionally, modern software paradigms have evolved as well. Complex design patterns have become the norm for experienced developers, immense libraries and frameworks are accepted as a prerequisite for any non-trivial software, and over-involved build tools often make even the task of understanding all of the program's dependencies a challenge.

It comes as no surprise that any kind of static analysis has struggled to keep up with this ever-increasing complexity both in programming languages and software. Even the seemingly simple task of computing a program's call-graph (i.e., which methods are called at every invocation site) requires sophisticated analysis for achieving acceptable precision. Thus, the main emphasis of pointer analysis algorithms is on combining fairly precise modeling of pointer behavior and memory abstractions with scalability.

\paragraph*{Thesis.}
\begin{displayquote}
\emph{Precise} yet \emph{scalable} static pointer analysis algorithms can be obtained by careful choice of different policies for different parts of the program. In a complementary fashion, analyses can be designed to offer (uniquely \todo{}) \emph{strong guarantees} on the soundness of results, but for a part of the program only.
%It is possible to implement \emph{highly sophisticated} and \emph{precise} static pointer analysis algorithms without forgoing \emph{scalability}. Furthermore, precision and the accompanied \emph{confidence in results} is not a global property of a given algorithm, but can be tweaked differently for different parts of the program.
\end{displayquote}

We provide a number of techniques for implementing scalable static pointer and alias analyses in the setting of Java programs by configuring the analysis strategy differently for different code parts. Additionally, we present a couple of defensive algorithms for reporting high-confidence results even in the presence of hostile or unknown program points.


\section{Pointer Analysis Crash Course}

Before enumerating the scientific contributions of this dissertation, it is mandatory to introduce certain concepts related to pointer analysis, that comprise the scientific and technical base of this work. This is by no means a detailed presentation of said concepts---a more elaborate introduction will follow in Chapter~\ref{chapter:background}.

\paragraph*{Implementation Platform \& Target Language.}
Most of the following work and algorithms have been expressed in the \doop{} framework \cite{oopsla:2009:Bravenboer}. \doop{} is a well established pointer analysis framework offering a wide variety of full-fledged algorithms for static pointer analysis of Java programs. More in Section~\ref{sec:back:doop}.


\paragraph*{Context Sensitivity.}
Implementing any sophisticated pointer analysis algorithm quickly turns out to be a balancing act between precision and performance tradeoffs. Any attempt for a scalable algorithm might inadvertently be accompanied by significant precision losses whereas an endeavour for highly precise results might also enforce huge performance penalties.

Throughout the years, the scientific community has amassed a few tools in its arsenal in order to tackle this conundrum of precision versus performance. Among those tools, a widely employed notion, that aims to improve precision without having to pay an unbearable performance cost, is that of \emph{context} resulting in \emph{context-sensitive} algorithms. An algorithm will use additional information (also known as context) to annotate analysis components with the aim of countering potential precision losses. The key idea is that the analysis will differentiate the handling of program elements under some contexts while it will collapse it under others. For instance, an algorithm might differentiate the analysis of a method when called from method \code{A} or method \code{B} or anywhere else (thus under three different contexts).

Two main kinds of context have been widely used in the past; in \emph{call-site-sensitive} analyses call instructions comprise the context elements, whereas in \emph{object-sensitive} analyses context is based on the identity of the calling object at each method invocation. More in Section~\ref{sec:back:context}.


\paragraph*{May vs. Must Analyses.}
The goal of any static program analysis algorithm is to reason about a set of behaviors under all potential program executions. This endeavour is an undecidable problem for any set of behaviors other than the most trivial ones. As a consequence, any practical algorithm has to \emph{approximate} results in one of two directions; either \emph{over}-approximate and both report all possible behaviors and also some that will never actually arise, or to \emph{under}-approximate and be conservative by reporting only a subset of potential arising behaviors. Analyses are often categorized as \emph{may}-analyses when they over-approximate results, and as \emph{must}-analyses when they under-approximate results. More in Section~\ref{sec:back:may-must}.


\paragraph*{Soundness.}
A formal term often used to accompany static pointer analysis algorithms is that of \emph{soundness}. In layman's terms, an algorithm is said to be sound when it actually does what it claims. For instance, a may-pointer analysis claims that it aims to over-approximate the set of objects that various program expressions may point to in all possible program executions. If the results are not missing any such inference that could arise in a program execution, then the algorithm is sound. Due to various factors, most may-pointer analysis algorithms forgo soundness in order to maintain scalability. A more detailed discussion regarding soundness will follow in Sections~\ref{sec:back:soundness}-\ref{sec:back:soundiness}.



\section{Scientific Contributions}

In this section, we will briefly explain the main scientific contributions of this dissertation. As already mentioned, the exploration happens in the context of analyzing Java---mainly by use of the \doop{} framework---although it is not far-fetched to generalize results to other languages that offer similar features and follow similar paradigms.

Ever since the introduction of object sensitivity by Milanova et al.~\cite{issta:2002:Milanova}, there has been increasing evidence that it is the superior context choice for programs expressed in object-oriented languages, yielding a high precision to cost ratio. Such has been its success that in practice it has almost superseded the use of more traditional call-site-sensitive analyses in object-oriented languages. Nevertheless, a call-site-sensitive analysis is not always inferior as there are language features and code patterns that may partially favor this kind of context abstraction.

Consequently, one might consider an approach where both context flavors are---naively---combined in every program point with the goal of increasing the precision of the end result. Truly, such a combination would bear some precision benefits but in most cases it would be accompanied by an infeasibly high cost.

\paragraphhead{First contribution.} Our first scientific contribution is a step towards a more sophisticated handling, aiming to achieve a beneficial combination of both context flavors. We propose a \emph{hybrid} context flavor for defining a family of analyses where classical contexts are mixed and combined only in those program points where it is profitable for the analysis. The resulting selective combination of both context kinds vastly outperforms not only analyses following the naive non-selective combination approach, but also their ``normal'' object-sensitive counterparts. This result holds for a large array of context-sensitive analyses establishing a new set of performance/precision sweet spots.

\paragraphhead{Second contribution.} The second scientific contribution tries to tackle an oft-reported issue with context-sensitive analyses, in that they mostly operate in two extremes: either the analysis is precise enough that it manipulates only manageable sets of data, and thus scales impressively well, or the analysis gets quickly derailed at the first sign of---massive---imprecision and becomes orders-of-magnitude more expensive than would be expected given the program's size. Currently, there is no approach for a \emph{precise}, context-\emph{sensitive} (of any context flavor) analysis that would scale across the board at a level comparable to that of a context-\emph{insensitive} one. Instead, we propose a two step process by means of \emph{introspective analysis}: the approach uniformly scales context-sensitive analyses by eliminating the performance-detrimental behavior, only at a small precision expense.

Introspective analysis employs a common adaptive pattern: it first performs a context-insensitive analysis and then it uses the results to selectively refine (i.e., analyze context-sensitively) only those program elements that are expected not to cause an explosion in running time or memory space. The technical challenge is to appropriately identify such program elements. We show that a simple but principled approach can be remarkably effective, achieving scalability (often with dramatic speedup) for benchmarks previously completely out-of-reach for deep context-sensitive analyses.

For the last two contributions, we shift our attention towards analyses that aim for the highest confidence in their claims. Although quite reluctant and conservative in making a claim, when they actually do they make certain that it is the correct decision.

\paragraphhead{Third contribution.} The next, third, contribution features a different flavor of static program analysis. Instead of the more commonly researched paradigm of \emph{may}-analyses, we chose to explore the alternative approach of a \emph{must}-analysis. More specifically, we focus on an instance of a \emph{must-alias} (also known as \emph{definite-alias}) analysis that aims to infer aliasing relationships among program expressions that are guaranteed to always hold.\footnote{As previously mentioned, a must-analysis will aim to compute an under-approximation of behaviors that will happen in every possible program execution.} The applications of a must-alias analysis are manifold:
\begin{inparaenum}[(1)]
\item it is useful for enabling optimizations such as constant folding and register allocation,
\item it can increase the precision of bug detectors, e.g., greatly benefiting a null-reference detector and a non-termination detector, and
\item it can be used internally as part of more complex analyses, e.g., one that can reason correctly about ``strong updates'' at instructions that modify the heap.
\end{inparaenum}
In order to compute high-confidence, non-trivial results, the analysis needs to be flow-sensitive, i.e., compute information at each program point and propagate it forward while respecting the control-flow of the program.

Furthermore, we observe that a must-alias analysis exhibits certain properties that can be exploited in order to achieve a more efficient algorithm without any compromise in the precision or the validity of its results. We present a custom specialized \emph{data structure} that speeds up a must-alias analysis by nearly two orders of magnitude. The data structure achieves its efficiency by encoding multiple alias sets in a single linked structure, and compactly representing the aliasing relations of arbitrarily long program expressions. Under this approach, must-alias analysis can be performed efficiently, over large Java benchmarks, in under half a minute, making the analysis cost acceptable for most practical uses.

\paragraphhead{Fourth contribution.} For our last contribution, we revisit the setting of a may-analysis but this time while aiming to explore the potential of a truly \emph{sound}---instead of just soundy---yet \emph{practical} analysis. We present such an approach in a \emph{defensive} may-point-to analysis, which can guarantee soundness even in the presence of arbitrary opaque code.\footnote{Code that cannot be analyzed such as dynamically generated or native code, or dynamic language features such as reflection, \code{invokedynamic}, etc.} A key design tenet of our approach is \emph{laziness}: the analysis computes points-to relationships only for program expressions that are guaranteed to never escape into opaque code.

The defensive nature of our analysis means that it might miss some valid inferences, but because of its laziness it will never waste work to compute sets that are not ``complete'', i.e. that may be missing elements due to opaque code. This frugal approach is what enables the great efficiency of the algorithm, allowing for a highly precise points-to analysis (such as a 5-call-site-sensitive, flow-sensitive analysis). Despite its conservative nature, the analysis yields sound, actionable results for a large subset of the program code, achieving (under worst-case assumptions) \nums{34-74\%} of the program coverage of an unsound state-of-the-art analysis for real-world programs.


\section{Outline}

The rest of this dissertation is organized as follows:
\begin{itemize}[$\bullet$]
\item Chapter~\ref{chapter:background} offers a quick yet non-trivial introduction to certain notions or properties that are important to take under consideration when designing a sophisticated static pointer analysis algorithm.

\item Chapter~\ref{chapter:hybrid} examines how a naive combination of object sensitivity and call-site sensitivity into a single analysis can be massively penalizing in terms of performance. Following that, we presents a hybrid context-sensitive approach for implementing points-to analyses that leverage the benefits of combining both object and a call-site sensitivity while avoiding to pay most of the cost of a naive combination.

This chapter presents research previously published in \emph{``Hybrid Context-Sensitivity for Points-To Analysis''}~\cite{pldi:2013:Kastrinis}.

\item Chapter~\ref{chapter:introspective} examines the well-known, bi-modal nature of classical static program points-to analyses in regards to scalability; they are either quite scalable or not scalable at all. In order to counter that discrepancy, we propose an adaptive approach in introspective analysis, where an imprecise analysis is used as a stepping stone in order to fine-tune program points in which a more precise handling is both beneficial and not detrimental to the overall analysis's performance.

This chapter presents research previously published in \emph{``Introspective Analysis: Context-sensitivity, Across the Board''}~\cite{pldi:2014:Smaragdakis}.
\end{itemize}

Both aforementioned contributions aim for more scalable analyses that achieve superior performance without foregoing precision. The next three contributions aim for analyses that although more restrained on what they report, they do so with much more confidence in the accuracy of their claims.

\begin{itemize}[$\bullet$]
\item Chapter~\ref{chapter:must-logic} examines how to compose a declarative model of a rich family of must-alias analyses, with emphasis on a careful and compact modeling, while at the same time exposing the key points where the algorithm's inference power can be adjusted.

This chapter presents research previously published in \emph{A Datalog Model of Must-Alias Analysis}~\cite{soap:2017:Balatsouras}.

\item Chapter~\ref{chapter:must-data} builds upon the previous chapter and goes forth to provide a specialized data structure that by exploiting the nature of a must-alias analysis it achieves high performance without any sacrifice on the accuracy of its results. We explore the data structure's performance in both an imperative (implemented in Java) and a declarative (implemented in Datalog) setting and contrast it extensively with prior techniques.

This chapter presents research previously published in \emph{An Efficient Data Structure for Must-Alias Analysis}~\cite{cc:2018:Kastrinis}.

\item Chapter~\ref{chapter:defensive} examines how a defensive reasoning in the presence of opaque code can be combined along with computational laziness in order to produce a highly efficient, highly precise and truly sound may-points-to analysis.

This chapter presents research previously published in \emph{Defensive Points-To Analysis: Effective Soundness via Laziness}~\cite{ecoop:2018:Smaragdakis}, that also received a \emph{Distinguished Paper} award.
\end{itemize}

\begin{itemize}[$\bullet$]
\item Chapter~\ref{chapter:related} first discusses related work that is specific to previous chapters, and then expands to various other interesting subjects in the broader realm of static analysis.

\item Chapter~\ref{chapter:conclusions} concludes this dissertation by assessing our initial thesis and discussing future work.
\end{itemize}